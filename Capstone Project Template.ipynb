{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Generate a report of the top 5 countries citizens that travel to most of visied the US airport city. This report will also show the population.\n",
    "\n",
    "### Please see the Readme file for more information. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "Please see the Readme file for more information.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "There are 4 data source (included CSV format and parquet format). We try to read those data to have a basic logic understand. It would help us for next step for data modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "# Read in the data here \n",
    "#1 Temperature  \n",
    "f_temperature = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "#2 Immigration  \n",
    "f_immigration_csv = 'immigration_data_sample.csv' \n",
    "f_immigration_sas = \"sas_data/part-00013-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\"\n",
    "f_immigration_folder = \"./sas_data\"\n",
    "#3 Airpot \n",
    "f_airport = 'airport-codes_csv.csv'\n",
    "#4 cities \n",
    "f_cities = 'us-cities-demographics.csv' \n",
    "#5 country from I94_SAS_Labels_Descriptions.SAS \n",
    "f_country = 'country.csv'\n",
    "\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    ".getOrCreate()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Gather Data | Immigration table \n",
    "Read only ONE specified file this time : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "|5748522.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20579.0|  57.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1959.0|10292016|     M|  null|     NZ|9.498180283E10|00010|      B2|\n",
      "|5748523.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20586.0|  66.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1950.0|10292016|     F|  null|     NZ|9.497968993E10|00010|      B2|\n",
      "|5748524.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20586.0|  41.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1975.0|10292016|     F|  null|     NZ|9.497974673E10|00010|      B2|\n",
      "|5748525.0|2016.0|   4.0| 245.0| 464.0|    HOU|20574.0|    1.0|     FL|20581.0|  27.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1989.0|10292016|     M|  null|     NZ|9.497324663E10|00028|      B2|\n",
      "|5748526.0|2016.0|   4.0| 245.0| 464.0|    LOS|20574.0|    1.0|     CA|20581.0|  26.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1990.0|10292016|     F|  null|     NZ|9.501354793E10|00002|      B2|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "235125\n"
     ]
    }
   ],
   "source": [
    "df_immigration = spark.read.parquet(f_immigration_sas) \n",
    "df_immigration.show(10)\n",
    "print(df_immigration.count())\n",
    "df_immigration.createOrReplaceTempView(\"immigration\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Gather Data | Temperature table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-04-01|5.7879999999999985|           3.6239999999999997|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-05-01|            10.644|           1.2830000000000001|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-06-01|14.050999999999998|                        1.347|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-07-01|            16.082|                        1.396|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-08-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "8599212\n"
     ]
    }
   ],
   "source": [
    "df_temperature = spark.read.option(\"header\", \"true\").csv(f_temperature)  \n",
    "df_temperature.show(10)\n",
    "print(df_temperature.count()) \n",
    "df_temperature.createOrReplaceTempView(\"temperature\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Gather Data ｜ Airport table  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "| 00AS|small_airport|      Fulton Airport|        1100|       NA|         US|     US-OK|        Alex|    00AS|     null|      00AS|-97.8180194, 34.9...|\n",
      "| 00AZ|small_airport|      Cordes Airport|        3810|       NA|         US|     US-AZ|      Cordes|    00AZ|     null|      00AZ|-112.165000915527...|\n",
      "| 00CA|small_airport|Goldstone /Gts/ A...|        3038|       NA|         US|     US-CA|     Barstow|    00CA|     null|      00CA|-116.888000488, 3...|\n",
      "| 00CL|small_airport| Williams Ag Airport|          87|       NA|         US|     US-CA|       Biggs|    00CL|     null|      00CL|-121.763427, 39.4...|\n",
      "| 00CN|     heliport|Kitchen Creek Hel...|        3350|       NA|         US|     US-CA| Pine Valley|    00CN|     null|      00CN|-116.4597417, 32....|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "55075\n"
     ]
    }
   ],
   "source": [
    "df_airport = spark.read.option(\"header\", \"true\").csv(f_airport)  \n",
    "df_airport.show(10)\n",
    "df_airport.createOrReplaceTempView(\"airport\") \n",
    "print(df_airport.count()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Gather Data ｜ Cities Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|         State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|      Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy| Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|       Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|    California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|    New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "|          Peoria|      Illinois|      33.1|          56229|            62432|          118661|              6634|        7517|                   2.4|        IL|American Indian a...| 1343|\n",
      "|        Avondale|       Arizona|      29.1|          38712|            41971|           80683|              4815|        8355|                  3.18|        AZ|Black or African-...|11592|\n",
      "|     West Covina|    California|      39.8|          51629|            56860|          108489|              3800|       37038|                  3.56|        CA|               Asian|32716|\n",
      "|        O'Fallon|      Missouri|      36.0|          41762|            43270|           85032|              5783|        3269|                  2.77|        MO|  Hispanic or Latino| 2583|\n",
      "|      High Point|North Carolina|      35.5|          51751|            58077|          109828|              5204|       16315|                  2.65|        NC|               Asian|11060|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "2891\n"
     ]
    }
   ],
   "source": [
    "df_cities = spark.read.option(\"header\", \"true\").option(\"sep\", \";\").csv(f_cities)  \n",
    "df_cities.show(10)\n",
    "print(df_cities.count()) \n",
    "df_cities.createOrReplaceTempView(\"cities\") "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Gather Data ｜ Country Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "|code|        country|\n",
      "+----+---------------+\n",
      "| 582|        MEXICO |\n",
      "| 236|    AFGHANISTAN|\n",
      "| 101|        ALBANIA|\n",
      "| 316|        ALGERIA|\n",
      "| 102|        ANDORRA|\n",
      "| 324|         ANGOLA|\n",
      "| 529|       ANGUILLA|\n",
      "| 518|ANTIGUA-BARBUDA|\n",
      "| 687|     ARGENTINA |\n",
      "| 151|        ARMENIA|\n",
      "+----+---------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "289\n"
     ]
    }
   ],
   "source": [
    "df_country = spark.read.option(\"header\", \"true\").csv(f_country)  \n",
    "df_country.show(10)\n",
    "print(df_country.count()) \n",
    "df_country.createOrReplaceTempView(\"country\") "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps | Cities Table\n",
    "1. Field name has space `State Code`, use Backticks  ` ` select \n",
    "2. One city could has multiple race "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2891\n",
      "+----------------+------------+----------+------------+--------------------+-----------------+---------------+\n",
      "|            City|       State|State_Code|Foreign_Born|                Race|Female_Population|Male_Population|\n",
      "+----------------+------------+----------+------------+--------------------+-----------------+---------------+\n",
      "| North Las Vegas|      Nevada|        NV|       49815|American Indian a...|           118443|         116350|\n",
      "|      Costa Mesa|  California|        CA|       26645|               Asian|            54089|          59097|\n",
      "|     Kansas City|    Missouri|        MO|       37787|Black or African-...|           246931|         228430|\n",
      "|       Pawtucket|Rhode Island|        RI|       17884|American Indian a...|            35511|          36072|\n",
      "|        Columbus|        Ohio|        OH|      101129|  Hispanic or Latino|           435086|         413981|\n",
      "|Port Saint Lucie|     Florida|        FL|       34003|American Indian a...|            95341|          84069|\n",
      "|    Jacksonville|     Florida|        FL|       85650|Black or African-...|           448828|         419203|\n",
      "|       San Diego|  California|        CA|      373842|Black or African-...|           701081|         693826|\n",
      "|            Erie|Pennsylvania|        PA|        6275|               Asian|            50852|          48618|\n",
      "|       San Ramon|  California|        CA|       29691|               Asian|            36911|          39221|\n",
      "+----------------+------------+----------+------------+--------------------+-----------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlstr = \"select City, State, cast(`State Code` as varchar(10) ) State_Code, cast(`Foreign-born` as int ) Foreign_Born, Race, \\\n",
    "        `Female Population` Female_Population, `Male Population` Male_Population \\\n",
    "         from cities \\\n",
    "         group by City, State, State_Code, Foreign_Born, Race, Female_Population, Male_Population \"\n",
    "         #group by City, State, `State Code`, `Foreign-born`, Race, `Female Population`,`Male Population` \" \n",
    "\n",
    "df_cities = spark.sql(sqlstr) \n",
    "df_cities.createOrReplaceTempView(\"cities\") \n",
    "print(df_cities.count()) \n",
    "df_cities.show(10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Step | Airport Table\n",
    "local_code = PK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21213\n",
      "+----------+--------------------+----------+-------------+-------------+\n",
      "|local_code|                name|state_code|         type| municipality|\n",
      "+----------+--------------------+----------+-------------+-------------+\n",
      "|      02OH|   Zimmerman Airport|        OH|small_airport|      Fremont|\n",
      "|      08AK|      Fisher Airport|        AK|small_airport|     Big Lake|\n",
      "|      0AZ1|        Taylor Field|        AZ|small_airport|       Marana|\n",
      "|      0CA5|Hoffman Private A...|        CA|small_airport| Santa Ysabel|\n",
      "|      0IA0|Knoxville Area Co...|        IA|     heliport|    Knoxville|\n",
      "|       0N6|Albanna Aviation ...|        DE|small_airport|       Felton|\n",
      "|      0TS7|    Flying U Airport|        TX|small_airport|Mineral Wells|\n",
      "|      0XS9|        French Field|        TX|small_airport|      Bullard|\n",
      "|      11MA|    Bulljump Airport|        MA|small_airport|      Wareham|\n",
      "|      12CA|Faber Vineyards A...|        CA|small_airport|         Lodi|\n",
      "+----------+--------------------+----------+-------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sqlstr = \"select local_code, name, SPLIT(iso_region,'-')[1] as state_code, type, municipality from airport\\\n",
    " where  municipality is not null and iso_country ='US' and local_code is not null \\\n",
    " group by local_code, name, SPLIT(iso_region,'-')[1], type, municipality\" \n",
    " \n",
    "df_airport = spark.sql(sqlstr)\n",
    "df_airport.createOrReplaceTempView(\"airport\")\n",
    "print(df_airport.count()) \n",
    "df_airport.show(10) \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Step | Temperature Table\n",
    "Only need US temerature, but there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n",
      "+---------------+------------------+-----+\n",
      "|           City|AverageTemperature|count|\n",
      "+---------------+------------------+-----+\n",
      "|      Worcester| 7.341440525809558| 3119|\n",
      "|     Charleston|18.696557871112546| 3119|\n",
      "|         Corona| 16.12483712696008| 1977|\n",
      "|    Springfield|10.647931343609901| 9147|\n",
      "|          Tempe|  21.0487690509584| 2139|\n",
      "|North Las Vegas| 17.45498153547133| 2058|\n",
      "|       Thornton| 8.777836262323191| 2333|\n",
      "|        Phoenix|  21.0487690509584| 2139|\n",
      "|      Hollywood| 23.06892444289695| 2872|\n",
      "| Pembroke Pines| 23.06892444289695| 2872|\n",
      "+---------------+------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temperature.createOrReplaceTempView(\"temperature\")    \n",
    "sqlstr = \"select City, avg(AverageTemperature) AverageTemperature, count(*) count\\\n",
    " from temperature \\\n",
    " where country='United States' and AverageTemperature is not null  \\\n",
    " group by City \" \n",
    "df_temperature = spark.sql(sqlstr)\n",
    "df_temperature.createOrReplaceTempView(\"temperature\")\n",
    "print(df_temperature.count())\n",
    "df_temperature.show(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Step | Immigration Table\n",
    "1. convert dataype using cast command  \n",
    "2. immigration.i94port => airport.local_code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1102499\n",
      "+------+------+-------+-------+-------+------+-------+--------+------+-----+\n",
      "|i94cit|i94res|i94port|i94addr|biryear|gender|i94visa|visatype|I94BIR|count|\n",
      "+------+------+-------+-------+-------+------+-------+--------+------+-----+\n",
      "|   209|   209|    HHW|     HI|   1988|     F|      2|      WT|    28| 1141|\n",
      "|   209|   209|    HHW|     HI|   1986|     F|      2|      WT|    30| 1123|\n",
      "|   209|   209|    HHW|     HI|   1987|     F|      2|      WT|    29| 1069|\n",
      "|   209|   209|    HHW|     HI|   1989|     F|      2|      WT|    27| 1057|\n",
      "|   209|   209|    HHW|     HI|   1985|     F|      2|      WT|    31| 1019|\n",
      "|   209|   209|    HHW|     HI|   1984|     F|      2|      WT|    32|  982|\n",
      "|   209|   209|    HHW|     HI|   1990|     F|      2|      WT|    26|  968|\n",
      "|   209|   209|    HHW|     HI|   1983|     F|      2|      WT|    33|  872|\n",
      "|   209|   209|    HHW|     HI|   1986|     M|      2|      WT|    30|  815|\n",
      "|   209|   209|    HHW|     HI|   1987|     M|      2|      WT|    29|  800|\n",
      "+------+------+-------+-------+-------+------+-------+--------+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration = spark.read.parquet(f_immigration_folder)   \n",
    "df_immigration.createOrReplaceTempView(\"immigration\")  \n",
    "sqlstr = \"select cast(i94cit as int) i94cit, cast(i94res as int) , i94port, i94addr, cast(biryear as int), \\\n",
    "    gender, cast(i94visa as int), visatype, cast(I94BIR as int), count(*) count\\\n",
    "    from immigration \\\n",
    "    where i94addr is not null \\\n",
    "    group by i94cit, i94res, i94port, i94addr, biryear, gender, i94visa, visatype, I94BIR\\\n",
    "    order by count desc\" \n",
    "df_immigration = spark.sql(sqlstr)  \n",
    "df_immigration.createOrReplaceTempView(\"immigration\")\n",
    "print(df_immigration.count())\n",
    "df_immigration.show(10)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Fact table :\n",
    "immigration table\n",
    "Each day save one file to floder name YYYYMMDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "dateTimeObj = datetime.now()\n",
    "dateStr = f\"{dateTimeObj.year}{dateTimeObj.month}{dateTimeObj.day}\"  \n",
    "df_immigration.write.mode('overwrite').parquet(f\"model_immigration/{dateStr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Dimmension table : \n",
    "airport, cities, temperature, country tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport.write.mode('overwrite').parquet(\"model_airport\")\n",
    "df_temperature.write.mode('overwrite').parquet(\"model_temperature\")\n",
    "df_cities.write.mode('overwrite').parquet(\"model_cities\")\n",
    "df_country.write.mode('overwrite').parquet(\"model_country\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Reporting 1/2 : Top 5 country travel to US related infromation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------+------------+\n",
      "|i94cit|country       |i94port|visted_count|\n",
      "+------+--------------+-------+------------+\n",
      "|135   |UNITED KINGDOM|NYC    |7692        |\n",
      "|111   |FRANCE        |NYC    |5758        |\n",
      "|245   |CHINA, PRC    |CHI    |4892        |\n",
      "|135   |UNITED KINGDOM|ATL    |4886        |\n",
      "|135   |UNITED KINGDOM|CHI    |4648        |\n",
      "|245   |CHINA, PRC    |LOS    |4619        |\n",
      "|213   |INDIA         |NYC    |4599        |\n",
      "|135   |UNITED KINGDOM|LOS    |4443        |\n",
      "|135   |UNITED KINGDOM|NEW    |4347        |\n",
      "|245   |CHINA, PRC    |NYC    |4233        |\n",
      "+------+--------------+-------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "637\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "#top 5 country \n",
    "sqlstr = \" select i94cit, c.country, i94port,count(*) visted_count \\\n",
    "        from immigration i \\\n",
    "        join country c on i.i94cit = c.code \\\n",
    "        where i.i94cit in (  select i94cit  \\\n",
    "                                            from immigration group by i94cit order by count(*) desc limit 5 )\\\n",
    "        group by i94cit, c.country, i94port \\\n",
    "        order by count(*) desc\"\n",
    "df_top5countries = spark.sql(sqlstr)\n",
    "df_top5countries.createOrReplaceTempView(\"top5countries\")  \n",
    "df_top5countries.show(10, False) \n",
    "print(df_top5countries.count() ) \n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Reporting 2/2 : Top 5 country travel to US related infromation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------------------------------------+------------+----------+------------+\n",
      "|country       |name                                            |visted_count|state     |foreign_born|\n",
      "+--------------+------------------------------------------------+------------+----------+------------+\n",
      "|UNITED KINGDOM|Hartsfield Jackson Atlanta International Airport|4886        |Georgia   |738925      |\n",
      "|UNITED KINGDOM|Lakefront Airport                               |4347        |Louisiana |417095      |\n",
      "|UNITED KINGDOM|Miami International Airport                     |4084        |Florida   |7845566     |\n",
      "|FRANCE        |Hartsfield Jackson Atlanta International Airport|3221        |Georgia   |738925      |\n",
      "|UNITED KINGDOM|Orlando Executive Airport                       |3009        |Florida   |7845566     |\n",
      "|FRANCE        |Miami International Airport                     |2870        |Florida   |7845566     |\n",
      "|CHINA, PRC    |Seattle Tacoma International Airport            |2656        |Washington|2204810     |\n",
      "|INDIA         |Lakefront Airport                               |2651        |Louisiana |417095      |\n",
      "|CHINA, PRC    |Dallas Love Field                               |2587        |Texas     |14498054    |\n",
      "|CHINA, PRC    |Coleman A. Young Municipal Airport              |2498        |Michigan  |1214547     |\n",
      "+--------------+------------------------------------------------+------------+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "278\n"
     ]
    }
   ],
   "source": [
    "sqlstr = \"select t.country, a.name, t.visted_count, c.state, sum(  c.foreign_born)  foreign_born  \\\n",
    "           from top5countries t \\\n",
    "           join airport a \\\n",
    "           on t.i94port = a.local_code \\\n",
    "           join cities c \\\n",
    "           on a.state_code = c.state_code \\\n",
    "           group by t.country, a.name, t.visted_count, c.state \"\n",
    "    \n",
    "df_top5reporting = spark.sql(sqlstr)\n",
    "df_top5reporting.show(10, False)\n",
    "print(df_top5reporting.count() )\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "qa_airport = spark.read.parquet(\"model_airport\")\n",
    "qa_cities = spark.read.parquet(\"model_cities\")\n",
    "qa_temperature = spark.read.parquet(\"model_temperature\") \n",
    "qa_country = spark.read.parquet(\"model_country\") \n",
    "qa_immigration = spark.read.parquet(f\"model_immigration/{dateStr}\")  \n",
    "\n",
    "qa_immigration.createOrReplaceTempView(\"qa_immigration\")  \n",
    "qa_cities.createOrReplaceTempView(\"qa_cities\") \n",
    "qa_airport.createOrReplaceTempView(\"qa_airport\") \n",
    "qa_temperature.createOrReplaceTempView(\"qa_temperature\") \n",
    "qa_country.createOrReplaceTempView(\"qa_country\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fn_zeroRecord(tables):   \n",
    "    for table in tables:\n",
    "        print (table.count() > 0) \n",
    "    \n",
    "\n",
    "def fn_nullValue(spark, checking ):\n",
    "    for table in checking:\n",
    "        print(f\"Performing data quality check on table {table}...\")\n",
    "        for column in checking[table]:\n",
    "            returnedVal = spark.sql(f\"\"\"SELECT COUNT(*) as nbr FROM {table} WHERE {column} IS NULL\"\"\")\n",
    "            if returnedVal.head()[0] > 0:\n",
    "                raise ValueError(f\"Data quality check failed! Found NULL values in {column} column!\")\n",
    "        print(f\"Table {table} passed.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    " \n",
    "qa_airport = spark.read.parquet(\"model_airport\")\n",
    "qa_cities = spark.read.parquet(\"model_cities\")\n",
    "qa_temperature = spark.read.parquet(\"model_temperature\")\n",
    "qa_immigration = spark.read.parquet(f\"model_immigration/{dateStr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Performing data quality check on table qa_immigration...\n",
      "Table qa_immigration passed.\n",
      "Performing data quality check on table qa_airport...\n",
      "Table qa_airport passed.\n",
      "Performing data quality check on table qa_cities...\n",
      "Table qa_cities passed.\n"
     ]
    }
   ],
   "source": [
    "tables =  [ qa_airport, qa_cities, qa_temperature, qa_immigration ] \n",
    "checking_dict = { 'qa_immigration' : ['i94port'], 'qa_airport': ['local_code'],'qa_cities':['state_code'] } \n",
    "fn_zeroRecord(tables) \n",
    "fn_nullValue(spark, checking_dict ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "1. immigration\n",
    "- i94cit : country of citizenship\n",
    "- i94res : country of residence\n",
    "- i94port: arrival airport\n",
    "- arrdate: arrival date.\n",
    "- gender :female / male\n",
    "- i94visa: visa type\n",
    "\n",
    "2. cities\n",
    "- city --> city name \n",
    "- state--> state name\n",
    "- female_population -> female population number\n",
    "- male_population -> male population number\n",
    "- Race ->asian, back , white\n",
    "- foreign_born \n",
    "\n",
    "3.Tempeatrure \n",
    "- city -->city name \n",
    "- averageTempeatrure\n",
    "\n",
    "4.airports \n",
    "- local_code : airport code \n",
    "- name: airport  name\n",
    "- state_code : state code\n",
    "- type : airport type\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### model path\n",
    "1. model_immigration/YYYYMMDD/__.parquet\n",
    "2. model_airport/___.parquet\n",
    "3. model_cities/___.parquet\n",
    "4. model_country/___.parquet\n",
    "5. model_temperature/___.parquet\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
