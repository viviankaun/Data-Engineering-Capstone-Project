{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Decide on data processing tools, COPY command for \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "from pyspark.sql import SparkSession \n",
    "import configparser \n",
    "# CONFIG\n",
    "# config = configparser.ConfigParser()\n",
    "# config.read('config.cfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? \n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "toggleable": false,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-a7a7b733": {
       "bashCommand": "",
       "filesToOpen": [],
       "isPreviewButton": false,
       "runInBackground": false,
       "style": "secondary",
       "text": "",
       "toggleOffText": "HIDE SOLUTION",
       "toggleOnText": "SHOW SOLUTION"
      }
     }
    }
   },
   "source": [
    "#### Vivian Note:\n",
    "There are 4 types of files. We should reead those data to have a basic logic understand. \n",
    "It would help us for next step for data modeling.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here \n",
    "#1 Temperature  \n",
    "f_temperature = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "#2 Immigration  \n",
    "f_immigration_csv = 'immigration_data_sample.csv' \n",
    "f_immigration_sas = \"sas_data/part-00013-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\"\n",
    "f_immigration_folder = \"./sas_data\"\n",
    "#3 Airpot \n",
    "f_airport = 'airport-codes_csv.csv'\n",
    "#4 cities \n",
    "f_cities = 'us-cities-demographics.csv' \n",
    "#5 I94_SAS_Labels_Descriptions.SAS \n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    ".getOrCreate()\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Immigration table \n",
    "read ONE specified file : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "|5748522.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20579.0|  57.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1959.0|10292016|     M|  null|     NZ|9.498180283E10|00010|      B2|\n",
      "|5748523.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20586.0|  66.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1950.0|10292016|     F|  null|     NZ|9.497968993E10|00010|      B2|\n",
      "|5748524.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20586.0|  41.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1975.0|10292016|     F|  null|     NZ|9.497974673E10|00010|      B2|\n",
      "|5748525.0|2016.0|   4.0| 245.0| 464.0|    HOU|20574.0|    1.0|     FL|20581.0|  27.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1989.0|10292016|     M|  null|     NZ|9.497324663E10|00028|      B2|\n",
      "|5748526.0|2016.0|   4.0| 245.0| 464.0|    LOS|20574.0|    1.0|     CA|20581.0|  26.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1990.0|10292016|     F|  null|     NZ|9.501354793E10|00002|      B2|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "235125\n"
     ]
    }
   ],
   "source": [
    "df_immigration = spark.read.parquet(f_immigration_sas) \n",
    "df_immigration.show(10)\n",
    "print(df_immigration.count())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-04-01|5.7879999999999985|           3.6239999999999997|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-05-01|            10.644|           1.2830000000000001|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-06-01|14.050999999999998|                        1.347|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-07-01|            16.082|                        1.396|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-08-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "8599212\n"
     ]
    }
   ],
   "source": [
    "df_temperature = spark.read.option(\"header\", \"true\").csv(f_temperature)  \n",
    "df_temperature.show(10)\n",
    "print(df_temperature.count()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Airport table  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "| 00AS|small_airport|      Fulton Airport|        1100|       NA|         US|     US-OK|        Alex|    00AS|     null|      00AS|-97.8180194, 34.9...|\n",
      "| 00AZ|small_airport|      Cordes Airport|        3810|       NA|         US|     US-AZ|      Cordes|    00AZ|     null|      00AZ|-112.165000915527...|\n",
      "| 00CA|small_airport|Goldstone /Gts/ A...|        3038|       NA|         US|     US-CA|     Barstow|    00CA|     null|      00CA|-116.888000488, 3...|\n",
      "| 00CL|small_airport| Williams Ag Airport|          87|       NA|         US|     US-CA|       Biggs|    00CL|     null|      00CL|-121.763427, 39.4...|\n",
      "| 00CN|     heliport|Kitchen Creek Hel...|        3350|       NA|         US|     US-CA| Pine Valley|    00CN|     null|      00CN|-116.4597417, 32....|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "55075\n"
     ]
    }
   ],
   "source": [
    "df_airport = spark.read.option(\"header\", \"true\").csv(f_airport)  \n",
    "df_airport.show(10)\n",
    "df_airport.createOrReplaceTempView(\"airport\") \n",
    "print(df_airport.count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### Cities Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|         State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|      Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy| Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|       Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|    California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|    New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "|          Peoria|      Illinois|      33.1|          56229|            62432|          118661|              6634|        7517|                   2.4|        IL|American Indian a...| 1343|\n",
      "|        Avondale|       Arizona|      29.1|          38712|            41971|           80683|              4815|        8355|                  3.18|        AZ|Black or African-...|11592|\n",
      "|     West Covina|    California|      39.8|          51629|            56860|          108489|              3800|       37038|                  3.56|        CA|               Asian|32716|\n",
      "|        O'Fallon|      Missouri|      36.0|          41762|            43270|           85032|              5783|        3269|                  2.77|        MO|  Hispanic or Latino| 2583|\n",
      "|      High Point|North Carolina|      35.5|          51751|            58077|          109828|              5204|       16315|                  2.65|        NC|               Asian|11060|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "2891\n"
     ]
    }
   ],
   "source": [
    "df_cities = spark.read.option(\"header\", \"true\").option(\"sep\", \";\").csv(f_cities)  \n",
    "df_cities.show(10)\n",
    "print(df_cities.count()) \n",
    "df_cities.createOrReplaceTempView(\"cities\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "####Cities\n",
    "1. Field name has space `State Code`, use Backticks  ` ` select \n",
    "2. One city could has multiple race "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2891\n",
      "+----------------+--------------+----------+--------------------+-----+\n",
      "|            City|         State|State_Code|                Race|count|\n",
      "+----------------+--------------+----------+--------------------+-----+\n",
      "|          Quincy| Massachusetts|        MA|               White|    1|\n",
      "|      Wilmington|North Carolina|        NC|               Asian|    1|\n",
      "|           Tampa|       Florida|        FL|  Hispanic or Latino|    1|\n",
      "|        Gastonia|North Carolina|        NC|               Asian|    1|\n",
      "|           Tyler|         Texas|        TX|American Indian a...|    1|\n",
      "|          Rialto|    California|        CA|Black or African-...|    1|\n",
      "|           Sandy|          Utah|        UT|American Indian a...|    1|\n",
      "|    Arden-Arcade|    California|        CA|Black or African-...|    1|\n",
      "|          Upland|    California|        CA|Black or African-...|    1|\n",
      "|      Cape Coral|       Florida|        FL|Black or African-...|    1|\n",
      "|   Coral Springs|       Florida|        FL|American Indian a...|    1|\n",
      "|     Little Rock|      Arkansas|        AR|               Asian|    1|\n",
      "|     Bloomington|     Minnesota|        MN|Black or African-...|    1|\n",
      "|     Santa Maria|    California|        CA|  Hispanic or Latino|    1|\n",
      "|      Buena Park|    California|        CA|               White|    1|\n",
      "|           Nampa|         Idaho|        ID|  Hispanic or Latino|    1|\n",
      "|Sterling Heights|      Michigan|        MI|               White|    1|\n",
      "|            Napa|    California|        CA|  Hispanic or Latino|    1|\n",
      "|          Canton|          Ohio|        OH|American Indian a...|    1|\n",
      "|          Laredo|         Texas|        TX|  Hispanic or Latino|    1|\n",
      "|          Sparks|        Nevada|        NV|Black or African-...|    1|\n",
      "|            Kent|    Washington|        WA|               Asian|    1|\n",
      "|         Greeley|      Colorado|        CO|Black or African-...|    1|\n",
      "|          Renton|    Washington|        WA|  Hispanic or Latino|    1|\n",
      "|         Compton|    California|        CA|               Asian|    1|\n",
      "|         Lubbock|         Texas|        TX|American Indian a...|    1|\n",
      "|         Lubbock|         Texas|        TX|               White|    1|\n",
      "|        Stamford|   Connecticut|        CT|Black or African-...|    1|\n",
      "|    Fort Collins|      Colorado|        CO|               Asian|    1|\n",
      "|     Sioux Falls|  South Dakota|        SD|               Asian|    1|\n",
      "|        Beaumont|         Texas|        TX|  Hispanic or Latino|    1|\n",
      "|           Chico|    California|        CA|               Asian|    1|\n",
      "|    Lee's Summit|      Missouri|        MO|Black or African-...|    1|\n",
      "|         Wichita|        Kansas|        KS|               White|    1|\n",
      "|          Topeka|        Kansas|        KS|  Hispanic or Latino|    1|\n",
      "|      Cedar Park|         Texas|        TX|  Hispanic or Latino|    1|\n",
      "|       Fairfield|    California|        CA|Black or African-...|    1|\n",
      "|            Kent|    Washington|        WA|  Hispanic or Latino|    1|\n",
      "|       Elizabeth|    New Jersey|        NJ|  Hispanic or Latino|    1|\n",
      "|Rancho Cucamonga|    California|        CA|               White|    1|\n",
      "|     Federal Way|    Washington|        WA|               Asian|    1|\n",
      "|          Clovis|    California|        CA|  Hispanic or Latino|    1|\n",
      "| College Station|         Texas|        TX|  Hispanic or Latino|    1|\n",
      "|          Topeka|        Kansas|        KS|               Asian|    1|\n",
      "|        Bellevue|    Washington|        WA|               Asian|    1|\n",
      "|           Flint|      Michigan|        MI|  Hispanic or Latino|    1|\n",
      "|  Pembroke Pines|       Florida|        FL|  Hispanic or Latino|    1|\n",
      "|          Carson|    California|        CA|               White|    1|\n",
      "|    Johnson City|     Tennessee|        TN|               Asian|    1|\n",
      "+----------------+--------------+----------+--------------------+-----+\n",
      "only showing top 49 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sqlstr = \"select City, State, cast(`State Code` as varchar(10) ) State_Code, Race, Count(*) count from cities\\\n",
    "        group by City, State,`State Code`, Race\\\n",
    "        order by count desc \"\n",
    "\n",
    "df_cities = spark.sql(sqlstr)\n",
    "print(df_cities.count())\n",
    "df_cities.show(49)\n",
    "\n",
    "# sqlstr = \"select City, State, Count(*) count from cities\\\n",
    "#         group by City, State \\\n",
    "#         order by count desc \"\n",
    "# df_cities = spark.sql(sqlstr)\n",
    "# print(df_cities.count())\n",
    "\n",
    "# sqlstr = \"select * from cities where city='Austin' \" \n",
    "# df_cities = spark.sql(sqlstr)\n",
    "# print(df_cities.count())\n",
    "# df_cities.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### Airport \n",
    "local_code = PK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21213\n",
      "+----------+--------------------+----------+-------------+-------------+-----+\n",
      "|local_code|                name|state_code|         type| municipality|count|\n",
      "+----------+--------------------+----------+-------------+-------------+-----+\n",
      "|       MNG|Montana ARNG Heli...|        MT|       closed|     Billings|    2|\n",
      "|       EDC|Austin Executive ...|        TX|small_airport|       Austin|    2|\n",
      "|      08AK|      Fisher Airport|        AK|small_airport|     Big Lake|    1|\n",
      "|      0CA5|Hoffman Private A...|        CA|small_airport| Santa Ysabel|    1|\n",
      "|      0AZ1|        Taylor Field|        AZ|small_airport|       Marana|    1|\n",
      "|      0XS9|        French Field|        TX|small_airport|      Bullard|    1|\n",
      "|      0IA0|Knoxville Area Co...|        IA|     heliport|    Knoxville|    1|\n",
      "|       0N6|Albanna Aviation ...|        DE|small_airport|       Felton|    1|\n",
      "|      0TS7|    Flying U Airport|        TX|small_airport|Mineral Wells|    1|\n",
      "|      11MA|    Bulljump Airport|        MA|small_airport|      Wareham|    1|\n",
      "+----------+--------------------+----------+-------------+-------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#checking duplicate\n",
    "sqlstr = \"select local_code, name, SPLIT(iso_region,'-')[1] as state_code, type, municipality, count(*) count from airport\\\n",
    " where  municipality is not null and iso_country ='US' and local_code is not null \\\n",
    " group by local_code, name, SPLIT(iso_region,'-')[1], type, municipality\\\n",
    " order by count desc\" \n",
    "df_airport = spark.sql(sqlstr) \n",
    "print(df_airport.count()) \n",
    "df_airport.show(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    " #### Temperature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n",
      "+------------+-------------+------------------+-----+\n",
      "|        City|      Country|AverageTemperature|count|\n",
      "+------------+-------------+------------------+-----+\n",
      "| Springfield|United States|10.647931343609901| 9147|\n",
      "|    Columbus|United States|14.017228598909899| 6238|\n",
      "|      Aurora|United States| 9.423826269638306| 5474|\n",
      "|   Arlington|United States|14.542532329169752| 5444|\n",
      "|      Peoria|United States|14.423363446969729| 5280|\n",
      "|    Richmond|United States|13.971595172684463| 5096|\n",
      "|    Pasadena|United States| 18.22948024174803| 4302|\n",
      "|    Glendale|United States| 18.56516010689992| 4116|\n",
      "|     Buffalo|United States|7.7269057624960205| 3141|\n",
      "|Cedar Rapids|United States| 7.975768545049324| 3141|\n",
      "+------------+-------------+------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temperature.createOrReplaceTempView(\"temperature\")   \n",
    "df_temperature.columns\n",
    "#checking duplicate dataa \n",
    "sqlstr = \"select City, Country, avg(AverageTemperature) AverageTemperature, count(*) count\\\n",
    " from temperature \\\n",
    " where country='United States' and AverageTemperature is not null  \\\n",
    " group by City, Country \\\n",
    " order by count desc\" \n",
    "df_temperature = spark.sql(sqlstr) \n",
    "print(df_temperature.count())\n",
    "df_temperature.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "####immigration table\n",
    "convert dataype using cast command \n",
    "i94port =>airport.local_code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1102499\n",
      "+------+------+-------+-------+-------+------+-------+--------+------+-----+\n",
      "|i94cit|i94res|i94port|i94addr|biryear|gender|i94visa|visatype|I94BIR|count|\n",
      "+------+------+-------+-------+-------+------+-------+--------+------+-----+\n",
      "|   209|   209|    HHW|     HI|   1988|     F|      2|      WT|    28| 1141|\n",
      "|   209|   209|    HHW|     HI|   1986|     F|      2|      WT|    30| 1123|\n",
      "|   209|   209|    HHW|     HI|   1987|     F|      2|      WT|    29| 1069|\n",
      "|   209|   209|    HHW|     HI|   1989|     F|      2|      WT|    27| 1057|\n",
      "|   209|   209|    HHW|     HI|   1985|     F|      2|      WT|    31| 1019|\n",
      "|   209|   209|    HHW|     HI|   1984|     F|      2|      WT|    32|  982|\n",
      "|   209|   209|    HHW|     HI|   1990|     F|      2|      WT|    26|  968|\n",
      "|   209|   209|    HHW|     HI|   1983|     F|      2|      WT|    33|  872|\n",
      "|   209|   209|    HHW|     HI|   1986|     M|      2|      WT|    30|  815|\n",
      "|   209|   209|    HHW|     HI|   1987|     M|      2|      WT|    29|  800|\n",
      "+------+------+-------+-------+-------+------+-------+--------+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration = spark.read.parquet(f_immigration_folder)   \n",
    "df_immigration.createOrReplaceTempView(\"immigration\")  \n",
    "sqlstr = \"select cast(i94cit as int) i94cit, cast(i94res as int) , i94port, i94addr, cast(biryear as int), \\\n",
    "    gender, cast(i94visa as int), visatype, cast(I94BIR as int), count(*) count\\\n",
    "    from immigration \\\n",
    "    where i94addr is not null \\\n",
    "    group by i94cit, i94res, i94port, i94addr, biryear, gender, i94visa, visatype, I94BIR\\\n",
    "    order by count desc\" \n",
    "df_immigration = spark.sql(sqlstr)  \n",
    "print(df_immigration.count())\n",
    "df_immigration.show(10)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Fact table :\n",
    "immigration table\n",
    "Each day save one file to floder name YYYYMMDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "dateTimeObj = datetime.now()\n",
    "dateStr = f\"{dateTimeObj.year}{dateTimeObj.month}{dateTimeObj.day}\"  \n",
    "df_immigration.write.mode('overwrite').parquet(f\"model_immigration/{dateStr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Dimmension table : \n",
    "airport, cities, temperature tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport.write.mode('overwrite').parquet(\"model_airport\")\n",
    "df_temperature.write.mode('overwrite').parquet(\"model_temperature\")\n",
    "df_cities.write.mode('overwrite').parquet(\"model_cities\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def fn_zero(dataframe): \n",
    "    print (dataframe.count() ==0)\n",
    "\n",
    " \n",
    "qa_airport = spark.read.parquet(\"model_airport\")\n",
    "qa_cities = spark.read.parquet(\"model_cities\")\n",
    "qa_temperature = spark.read.parquet(\"model_temperature\")\n",
    "qa_immigration = spark.read.parquet(f\"model_immigration/{dateStr}\")\n",
    "tables =  [ qa_airport, qa_cities, qa_temperature, qa_immigration ] \n",
    "\n",
    "for tableObj in tables:\n",
    "    fn_zero(tableObj)\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### mode path\n",
    "model_immigration/YYYYMMDD/__.parquet\n",
    "model_airport/___.parquet\n",
    "model_cities/___.parquet\n",
    "modde_temperature/___.parquet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "-The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "-The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "airflow schedule "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "-The data was increased by 100x.\n",
    "if data source is on files system, I would like use COPY command insert into REDSHIFT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
